ProcessConfig = pFabMapV2
{
    //********************************************
	//             I/O Options
    //********************************************

	//Where is the Vocabulary?
	VocabPath = /home/lsilveira/Codigos/indigo_ws/src/dolphin_slam/experiments/yatch/training/
	VocabName = yatch

	//Where is the .oxs file?
    //This file is generated by WordMaker
    //and lists the bag-of-words for each image we're going to process
	DatasetPath = /home/lsilveira/Codigos/indigo_ws/src/dolphin_slam/experiments/yatch/training/
	DatasetName = yatch

	//Where to write out the resulting matrix of pdfs?
	//Results gets placed in a subdirectory of this path
	BaseOutputPath = /home/lsilveira/Codigos/indigo_ws/src/dolphin_slam/experiments/uwsim/test/Results/

    OutputFormat =  MATLAB   //Output format can be MATLAB, TXT_DENSE, TXT_SPARSE
                                //MATLAB option writes out results as a dense Matlab matrix.
                                //TXT_DENSE a a txt matrix format, recording every entry in the pdf
                                //TXT_SPARSE is a sparse txt matrix format, only recording entires with probability above SparseRecordingThreshold
    SparseRecordingThreshold = 0.001


    //********************************************
    //          Algorithm Parameters
    //********************************************
     //Mostly these parameters can be left unchanged
    //The only parameter that may need adjustment is P_OBSERVE_GIVEN_EXISTS
    //which defines the sensor model.

	//P_OBSERVE_GIVEN_EXISTS defines the true positive rate of our detector [p(z=1 | e = 1)].
        // E.g. a value of 0.3 means that when a feature exists in a scene, it is detected 30% of the time, and 70% of the time we have a false negative.
		//This is the crucial parameter. Results are very sensitive to getting this right.
		//0.35-0.45 seems to be a good model for our robot's camera. Lower values (0.3-0.4) might be more appropriate for hand-held video.
	P_OBSERVE_GIVEN_EXISTS		=	0.39

		//P_OBSERVE_GIVEN_NOT_EXISTS defines part of our detector model, p(z=1|e=0)
		//This is the crucial parameter. Results are very sensitive to getting this right.
	P_OBSERVE_GIVEN_NOT_EXISTS		=	0.0 // For car data set a value of 0.05 was used. 

		//P_AT_NEW_PLACE is our prior that a topological link with 
		//an unknown ednpoint leads to a new place.
		//The effect of this prior is normally small - overwhelmed by data likelihood.
	P_AT_NEW_PLACE				=	0.9	


		//LIKELIHOOD_SMOOTHING_FACTOR controls how we smooth the appearance likelihoods.
        //Valid values are in the range  0 to 1
        //
        //A value of 1 applies no smoothing
        //Values less than 1.0 smooth the observation likelihood. 
        //For details see See "FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance", Cummins and Newman, International Journal of Robotics Research
        //
        //Values less than 1 effectively mean that a sequence of matches is needed to trigger loop closure with high confidence.
        //We have found slight smoothing to be helpful in rejecting occasional outliers, while still detecting most loop closures.
	LIKELIHOOD_SMOOTHING_FACTOR	=	0.99 

		//RankingFunction defines the form of the likelihood term
		//
		//FABMAP_NAIVE_BAYES uses the FabMap model, with Naive Bayes likelihood term.
		//
		//FABMAP_CHOW_LIU uses the FabMap model, with Chow Liu likelihood term
	RankingFunction = FABMAP_CHOW_LIU
	
		//PriorModel defines how the position prior is generated
		//
		//UNIFORM_LOCATION_PRIOR throws away all position estimates from the last timestep
		//  and assumes any of the places in the map are equally likely.
		//
		//ALWAYS_MOVE_MOTION_MODEL takes the previous position estimate, and produces a prior
		//by transforming this position estimate through a motion model.
		//The motion model is that the robot randomly moves to one of the adjacent places.
		//
		//LEFT_RIGHT_BELOW_MOTION_MODEL takes the previous position estimate, and produces a prior
		//by transforming this position estimate through a motion model.
		//The motion model is that the robot randomly moves to 
		//one of the adjacent places, or remains where it is. All three actions have equal probability.
    PriorModel = UNIFORM_LOCATION_PRIOR

	
		//If the ALWAYS_MOVE_MOTION_MODEL motion model is defined, 
        	//this biases the motion in the forward direction.
		//1.0 means all the probability mass goes forward.
		//0.5 means that probability goes equally forward and backward.
		//Values less than 0.5 bias the motion backward.
    ForwardMotionBias = 0.5
       
	        //Disallow local matches
	        //Set the prior to be zero on the last N places, to eliminate sprurious matches due to view overlap with recent images. Comment out to disable.
	DisallowLocalMatches = 5 // 10 for car data set.

		//SamplerType selects the sampler used to determine the new place probability
		//
		//REAL_SAMPLES directly uses the images used to train the vocabulary as samples.
		//NULL_SAMPLER disables the sampling set entirely. The result then in an ML estimate over the places in the map. i.e the system performs localization
	SamplerType = REAL_SAMPLES 

        //How many samples to take when calculing p(Z)?
        //If using a data-based sampler (e.g REAL_SAMPLES), 
        //then there is a finite number of possible unique samples. 
        //If MinNumSamplesToTake is bigger than this, it gets reset to the max number of unique samples. 
	MinNumSamplesToTake = 10000

        //For the real sampler, you can optionally specify an OXS file to load as the sampling set.
        //By default (if none specified), the vocabulary training images are used as the sampling set.
        // RealSamplePath = /home/lsilveira/Codigos/indigo_ws/src/dolphin_slam/experiments/uwsim/training/
        // RealSampleFile = uwsim

		//Data association threshold.
		//If we allow data association, at what probability do we trigger it?
		//Currently Data Association not compiled into the build by default.
	DataAssociationThreshold = 0.999

        //Debugging options
        //If MaxNumberOfScenesToProcess is defined, we only process the first N scenes in a file
	//MaxNumberOfScenesToProcess = 100


    //**************************************************
    //          Optional: Input data subsampling
    //**************************************************

  //FabMap assumes consecutive images do not overlap in view, typically a meter or two apart.
  //If your data is already like this - great, ignore this section.
  //If not (e.g. your data is video), probably the easiest thing to do is subsample your data yourself before generating features. This will save you lots of unnecessary feature extraction time.
  // However, if you already have a fully processed dataset where the image frequency is too high, FabMap can do the subsampling for you here.
  KeyframeDetector = NONE
  //KeyframeDetectorParameter = 40      
                                        //Mode can be NONE, FIXED_INCREMENT, WORDS_IN_COMMON, or FABMAP_LIKELIHOOD
                                        //NONE              - No keyframes. Process every image.
                                        //FIXED_INCREMENT   - Every n-th frame, as specified by KeyframeDetectorParameter. Default 40.
                                        // The next two detectors attempt to be more clever, but can be a little tricky to tune. Don't try these unless you have no other way to subsample your input sensibly.
                                        //WORDS_IN_COMMON   - Pick next keyframe when number of visual words in common with last frame is below a threshold as specified by KeyframeDetectorParameter. Default 0.18.
                                        //FABMAP_LIKELIHOOD - New keyframe when FabMap likelihood suggests the current image is different from the last. This is expensive as each image requires a likelihood calculation
}
